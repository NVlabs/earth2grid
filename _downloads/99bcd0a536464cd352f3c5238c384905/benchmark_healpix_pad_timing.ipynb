{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmark different HEALPix padding implementations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\nimport torch\n\nfrom earth2grid import healpix\nfrom earth2grid.healpix import pad_backend\n\n# Print GPU information\nif torch.cuda.is_available():\n    print(\"CUDA available: Yes\")\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    print(f\"Device name: {torch.cuda.get_device_name()}\")\n    print(f\"Device count: {torch.cuda.device_count()}\")\n    print(f\"Device capability: {torch.cuda.get_device_capability()}\")\nelse:\n    print(\"CUDA not available\")\nprint(\"\\n\")\n\nnside = 128\npadding = nside // 2\nchannels = 384\ndtype = torch.float32\n\nneval = 10\n\n\ndef test_func(label, pad, compile=False):\n    # Reset memory stats and clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\n    # warm up\n    if compile:\n        pad = torch.compile(pad)\n    out = pad(p, padding=padding)\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(neval):\n        out = pad(p, padding=padding)\n    torch.cuda.synchronize()\n    stop = time.time()\n    gb_per_sec = out.nbytes * neval / (stop - start) / 1e9\n    peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024\n    label = label + \":\"\n    label = label + max(30 - len(label), 0) * \" \"\n    print(f\"{label} {gb_per_sec=:.2f} peak_memory={peak_memory:.2f}MB\")\n\n\nfor batch_size in [1, 2]:\n    p = torch.randn(size=(batch_size, 12, channels, nside, nside), dtype=dtype)\n    print(f\"Benchmarking results {neval=} {p.size()=} {padding=} {dtype=}\")\n\n    p = p.cuda()\n\n    with pad_backend(healpix.PaddingBackends.indexing):\n        test_func(\"Python\", healpix.pad)\n        test_func(\"Python + compile\", healpix.pad, compile=True)\n\n    with pad_backend(healpix.PaddingBackends.cuda):\n        test_func(\"HEALPix Pad\", healpix.pad)\n\n    with pad_backend(healpix.PaddingBackends.zephyr):\n        test_func(\"Zephyr pad\", healpix.pad)\n        print(\"Zephyr pad doesn't work well with torch.compile. Doesn't finish compiling.\")\n\n    p = torch.randn(size=(batch_size, 12 * nside * nside, channels), dtype=dtype).cuda()\n    test_func(\"Python: channels dim last*\", lambda x, padding: healpix.pad_with_dim(x, padding, dim=1), compile=False)\n    test_func(\n        \"Python + torch.compile: channels dim last*\",\n        lambda x, padding: healpix.pad_with_dim(x, padding, dim=1),\n        compile=True,\n    )\n    p_python_shape = p.shape\n\n    p = p.view(batch_size, 12, nside, nside, channels).permute(0, 1, 4, 2, 3)\n    with pad_backend(healpix.PaddingBackends.cuda):\n        test_func(\"HEALPix Pad: channels dim last\", healpix.pad)\n\n    print(\"\")\n\n\nprint(f\"* shape for Python channels dim last: {p_python_shape}\")\nprint(f\"* shape for HEALPix Pad channels dim last: {p.shape}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}